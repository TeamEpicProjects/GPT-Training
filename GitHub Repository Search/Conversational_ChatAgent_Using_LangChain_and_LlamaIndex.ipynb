{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8a7bc5f",
   "metadata": {},
   "source": [
    "## Creating Chat Assistant Using Llama index and LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ae9cb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27f4af2",
   "metadata": {},
   "source": [
    "### OpenAI API Key And GitHub Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e88193f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env OPENAI_API_KEY=sk-*******************************\n",
    "from llama_index import GPTVectorStoreIndex, GithubRepositoryReader\n",
    "from IPython.display import Markdown, display\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb80e77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env GITHUB_TOKEN=ghp_*********************************\n",
    "github_token = os.environ.get(\"GITHUB_TOKEN\")\n",
    "owner = \"dair-ai\"\n",
    "repo = \"Prompt-Engineering-Guide\"\n",
    "branch = \"main\"\n",
    "\n",
    "documents = GithubRepositoryReader(\n",
    "    github_token=github_token,\n",
    "    owner=owner,\n",
    "    repo=repo,\n",
    "    use_parser=False,\n",
    "    verbose=False,\n",
    ").load_data(branch=branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "998bad0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import download_loader, GPTVectorStoreIndex, ServiceContext, StorageContext\n",
    "service_context = ServiceContext.from_defaults(chunk_size_limit=512)\n",
    "\n",
    "storage_context = StorageContext.from_defaults()\n",
    "index = GPTVectorStoreIndex.from_documents(\n",
    "        documents, \n",
    "        service_context=service_context,\n",
    "        storage_context=storage_context,\n",
    "    )\n",
    "storage_context.persist(persist_dir=f'./storage/index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39cd585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import  load_index_from_storage\n",
    "storage_context = StorageContext.from_defaults(persist_dir=f'./storage/index')\n",
    "index = load_index_from_storage(storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbd52fe",
   "metadata": {},
   "source": [
    "### Creating Composable Graph Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5143887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import GPTListIndex, LLMPredictor, ServiceContext, load_graph_from_storage\n",
    "from langchain import OpenAI\n",
    "from llama_index.indices.composability import ComposableGraph\n",
    "\n",
    "index_summary = f\"OpenAI Cookbook repository\"\n",
    "\n",
    "# define an LLMPredictor set number of output tokens\n",
    "llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, max_tokens=512))\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n",
    "storage_context = StorageContext.from_defaults()\n",
    "\n",
    "# define a list index over the vector indices\n",
    "# allows us to synthesize information across each index\n",
    "graph = ComposableGraph.from_indices(\n",
    "    GPTListIndex,\n",
    "    [index], \n",
    "    index_summaries=[index_summary],\n",
    "    service_context=service_context,\n",
    "    storage_context = storage_context,\n",
    ")\n",
    "root_id = graph.root_id\n",
    "\n",
    "storage_context.persist(persist_dir=f'./storage/root')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81f6930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = load_graph_from_storage(\n",
    "    root_id=root_id, \n",
    "    service_context=service_context,\n",
    "    storage_context=storage_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2533f0",
   "metadata": {},
   "source": [
    "### Building Query Engine Using LangChain Chat_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c635aa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "from llama_index.langchain_helpers.agents import LlamaToolkit, create_llama_chat_agent, IndexToolConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62d04626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.query.query_transform.base import DecomposeQueryTransform\n",
    "decompose_transform = DecomposeQueryTransform(\n",
    "    llm_predictor, verbose=True\n",
    ")\n",
    "\n",
    "# define custom retrievers\n",
    "from llama_index.query_engine.transform_query_engine import TransformQueryEngine\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "query_engine = TransformQueryEngine(\n",
    "        query_engine,\n",
    "        query_transform=decompose_transform,\n",
    "        transform_extra_info={'index_summary': index.index_struct.summary},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "506620b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_config = IndexToolConfig(\n",
    "    query_engine=query_engine,\n",
    "    name=f\"Graph Index\",\n",
    "    description=\"useful for when you want to answer queries that require analyzing repo.\",\n",
    "    tool_kwargs={\"return_direct\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "263d7bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_config = IndexToolConfig(\n",
    "        query_engine=query_engine, \n",
    "        name=f\"Vector Index\",\n",
    "        description=f\"useful for when you want to answer queries about the repo\",\n",
    "        tool_kwargs={\"return_direct\": True}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5c111be",
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkit = LlamaToolkit(\n",
    "    index_configs= [tool_config] + [graph_config],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e05c05b",
   "metadata": {},
   "source": [
    "### Conversational Chat Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82911ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "llm=OpenAI(temperature=0)\n",
    "agent_chain = create_llama_chat_agent(\n",
    "    toolkit,\n",
    "    llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4f7f4976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Thought: Do I need to use a tool? No\n",
      "AI: Hi Ram, nice to meet you! How can I help you today?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hi Ram, nice to meet you! How can I help you today?'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain.run(input=\"hi, i am ram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5e212a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Thought: Do I need to use a tool? No\n",
      "AI: ChatGPT is an open-source natural language processing (NLP) model developed by OpenAI. It is based on the GPT-2 model and is designed to generate human-like text. Text-Davinci 002 is a text-generating model developed by OpenAI. It is based on the GPT-2 model and is designed to generate human-like text. Both models are powerful tools for natural language processing and can be used to generate text, answer questions, and provide insights into a wide range of topics.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ChatGPT is an open-source natural language processing (NLP) model developed by OpenAI. It is based on the GPT-2 model and is designed to generate human-like text. Text-Davinci 002 is a text-generating model developed by OpenAI. It is based on the GPT-2 model and is designed to generate human-like text. Both models are powerful tools for natural language processing and can be used to generate text, answer questions, and provide insights into a wide range of topics.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain.run(input=\"I'm trying to learn about chatgpt and recently worked with text davinci 002 model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9d2b822b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hello!, this is ram.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "AI: Hi Ram, nice to meet you! How can I help you today?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Agent: Hi Ram, nice to meet you! How can I help you today?\n",
      "User: Can you tell me about AI Cookbook?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "AI: The AI Cookbook is a collection of tutorials and examples for using AI technologies. It provides step-by-step instructions for building and training models, as well as tips and tricks for getting the most out of AI technologies. The Cookbook also includes a variety of sample code snippets and examples to help you get started quickly.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Agent: The AI Cookbook is a collection of tutorials and examples for using AI technologies. It provides step-by-step instructions for building and training models, as well as tips and tricks for getting the most out of AI technologies. The Cookbook also includes a variety of sample code snippets and examples to help you get started quickly.\n",
      "User: Can you give me some sample of code snippet?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "AI: Sure, here is a sample code snippet for using a deep learning model to classify images:\n",
      "\n",
      "```\n",
      "import tensorflow as tf\n",
      "\n",
      "# Load the data\n",
      "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
      "\n",
      "# Create the model\n",
      "model = tf.keras.models.Sequential([\n",
      "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
      "    tf.keras.layers.MaxPooling2D(2, 2),\n",
      "    tf.keras.layers.Flatten(),\n",
      "    tf.keras.layers.Dense(128, activation='relu'),\n",
      "    tf.keras.layers.Dense(10, activation='softmax')\n",
      "])\n",
      "\n",
      "# Compile the model\n",
      "model.compile(optimizer='adam',\n",
      "              loss='sparse_categorical_crossentropy',\n",
      "              metrics=['accuracy'])\n",
      "\n",
      "# Train the model\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Agent: Sure, here is a sample code snippet for using a deep learning model to classify images:\n",
      "\n",
      "```\n",
      "import tensorflow as tf\n",
      "\n",
      "# Load the data\n",
      "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
      "\n",
      "# Create the model\n",
      "model = tf.keras.models.Sequential([\n",
      "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
      "    tf.keras.layers.MaxPooling2D(2, 2),\n",
      "    tf.keras.layers.Flatten(),\n",
      "    tf.keras.layers.Dense(128, activation='relu'),\n",
      "    tf.keras.layers.Dense(10, activation='softmax')\n",
      "])\n",
      "\n",
      "# Compile the model\n",
      "model.compile(optimizer='adam',\n",
      "              loss='sparse_categorical_crossentropy',\n",
      "              metrics=['accuracy'])\n",
      "\n",
      "# Train the model\n",
      "User: Can you give me code to create chat model?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "AI: Sure, here is a sample code snippet for creating a chatbot using OpenAI's GPT-3 model:\n",
      "\n",
      "```\n",
      "import openai\n",
      "\n",
      "# Create the OpenAI API client\n",
      "openai = openai.API(\n",
      "    \"YOUR_API_KEY\",\n",
      "    \"YOUR_API_SECRET\"\n",
      ")\n",
      "\n",
      "# Create the GPT-3 model\n",
      "model = openai.Model.create(\n",
      "    engine=\"davinci\",\n",
      "    prompt=\"Hello, how can I help you?\"\n",
      ")\n",
      "\n",
      "# Generate text using the GPT-3 model\n",
      "response = model.generate(\n",
      "    max_tokens=256\n",
      ")\n",
      "\n",
      "# Print the generated text\n",
      "print(response.choices[0].text)\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Agent: Sure, here is a sample code snippet for creating a chatbot using OpenAI's GPT-3 model:\n",
      "\n",
      "```\n",
      "import openai\n",
      "\n",
      "# Create the OpenAI API client\n",
      "openai = openai.API(\n",
      "    \"YOUR_API_KEY\",\n",
      "    \"YOUR_API_SECRET\"\n",
      ")\n",
      "\n",
      "# Create the GPT-3 model\n",
      "model = openai.Model.create(\n",
      "    engine=\"davinci\",\n",
      "    prompt=\"Hello, how can I help you?\"\n",
      ")\n",
      "\n",
      "# Generate text using the GPT-3 model\n",
      "response = model.generate(\n",
      "    max_tokens=256\n",
      ")\n",
      "\n",
      "# Print the generated text\n",
      "print(response.choices[0].text)\n",
      "```\n",
      "User: Okay\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "AI: Sure, what can I do for you?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Agent: Sure, what can I do for you?\n",
      "User: Nothing.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAI: Alright, if you need any help, please don't hesitate to ask. Have a great day!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Agent: Alright, if you need any help, please don't hesitate to ask. Have a great day!\n",
      "User: Thank you\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    text_input = input(\"User: \")\n",
    "    if text_input == 'Thank you':\n",
    "        break\n",
    "    response = agent_chain.run(input=text_input)\n",
    "    print(f'Agent: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ef6b2e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hi, this is sai.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "AI: Hi Sai, nice to meet you! How can I help you today?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Agent: Hi Sai, nice to meet you! How can I help you today?\n",
      "User: I'm working on building some chatGPT models.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "AI: Great! What kind of chatGPT models are you building?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Agent: Great! What kind of chatGPT models are you building?\n",
      "User: I started working on GPT Turbo 3.5 by giving it custom input data.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "AI: Sounds interesting! What kind of custom input data are you giving GPT Turbo 3.5?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Agent: Sounds interesting! What kind of custom input data are you giving GPT Turbo 3.5?\n",
      "User: I tried giving custom input in format of git repo, pdfs and excel.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "AI: That's great! Using custom input data in the form of git repos, PDFs, and Excel files can help GPT Turbo 3.5 learn more effectively and provide more accurate results.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Agent: That's great! Using custom input data in the form of git repos, PDFs, and Excel files can help GPT Turbo 3.5 learn more effectively and provide more accurate results.\n",
      "User: Can you tell my name?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "AI: I'm sorry, I don't know your name.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Agent: I'm sorry, I don't know your name.\n",
      "User: I have mentioned it earlier\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "AI: Oh, you mentioned your name earlier. Is it Sai?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Agent: Oh, you mentioned your name earlier. Is it Sai?\n",
      "User: Yes.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "AI: Nice to meet you, Sai! Is there anything else I can help you with?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Agent: Nice to meet you, Sai! Is there anything else I can help you with?\n",
      "User: Thank you.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "AI: You're welcome! Is there anything else I can help you with?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Agent: You're welcome! Is there anything else I can help you with?\n",
      "User: Thank you\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    text_input = input(\"User: \")\n",
    "    if text_input == 'Thank you':\n",
    "        break\n",
    "    response = agent_chain.run(input=text_input)\n",
    "    print(f'Agent: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8674cc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987ab259",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15e8a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
