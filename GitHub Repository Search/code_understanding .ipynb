{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jId_2mcv5xEW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e2c3ae3-8f25-453f-8324-f45ec7cabb17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m789.1/789.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m476.8/476.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.7/72.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for deeplake (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip -q install openai\n",
        "!pip -q install tiktoken\n",
        "!pip -q install langchain\n",
        "!pip -q install deeplake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eo8Z6eLP560b"
      },
      "outputs": [],
      "source": [
        "\n",
        "# imports\n",
        "import ast  # for converting embeddings saved as strings back to arrays\n",
        "import openai  # for calling the OpenAI API\n",
        "import pandas as pd  # for storing text and embeddings data\n",
        "import tiktoken  # for counting tokens\n",
        "from scipy import spatial  # for calculating vector similarities for search\n",
        "\n",
        "\n",
        "# models\n",
        "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
        "GPT_MODEL = \"gpt-3.5-turbo\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpKUBZiq5_Ti",
        "outputId": "0423a9f4-c259-40f5-d900-c3d44557a2f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:··········\n",
            "Activeloop Token:··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import DeepLake\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')\n",
        "os.environ['ACTIVELOOP_TOKEN'] = getpass.getpass('Activeloop Token:')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6ZD9PWhKuaA"
      },
      "outputs": [],
      "source": [
        "embeddings = OpenAIEmbeddings(disallowed_special=())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2VruDd6K3Rk",
        "outputId": "4e028276-c3c1-4133-fc68-9ed8ce45fd1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Machine_Learning_A-Z'...\n",
            "remote: Enumerating objects: 243, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 243 (delta 1), reused 2 (delta 0), pack-reused 238\u001b[K\n",
            "Receiving objects: 100% (243/243), 11.10 MiB | 27.12 MiB/s, done.\n",
            "Resolving deltas: 100% (112/112), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/srafay/Machine_Learning_A-Z # replace any repository of your choice \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9xTnIF5d0ad"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "root_dir = '/content/Machine_Learning_A-Z'\n",
        "docs = []\n",
        "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
        "  n = 0\n",
        "  for file in filenames:\n",
        "    \n",
        "        try: \n",
        "            loader = TextLoader(os.path.join(dirpath, file), encoding='utf-8')\n",
        "            docs.extend(loader.load_and_split())\n",
        "            print \n",
        "        except Exception as e: \n",
        "            pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zTPGjbmeGM9"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "texts = text_splitter.split_documents(docs)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-BOX0YNX4SM",
        "outputId": "680b9e21-6f80-4c31-f02f-a90832c0c43b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2539"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwXOS4RdmWjA"
      },
      "outputs": [],
      "source": [
        "username = \"sarthmirashi\" # replace with your username from app.activeloop.ai\n",
        "db = DeepLake(dataset_path=f\"hub://{username}/MLAZ_RCTS\", embedding_function=embeddings, public=True) #dataset would be publicly available\n",
        "db.add_documents(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28LXEiuqoh3V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a667f4f-3a98-4c7a-e4ab-366e4023115e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/sarthmirashi/MLAZ_RCTS\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "-"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hub://sarthmirashi/MLAZ_RCTS loaded successfully.\n",
            "\n",
            "Deep Lake Dataset in hub://sarthmirashi/MLAZ_RCTS already exists, loading from the storage\n",
            "Dataset(path='hub://sarthmirashi/MLAZ_RCTS', read_only=True, tensors=['embedding', 'ids', 'metadata', 'text'])\n",
            "\n",
            "  tensor     htype      shape       dtype  compression\n",
            "  -------   -------    -------     -------  ------- \n",
            " embedding  generic  (2539, 1536)  float32   None   \n",
            "    ids      text     (2539, 1)      str     None   \n",
            " metadata    json     (2539, 1)      str     None   \n",
            "   text      text     (2539, 1)      str     None   \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r \r\r \r"
          ]
        }
      ],
      "source": [
        "db = DeepLake(dataset_path=\"hub://sarthmirashi/MLAZ_RCTS\", read_only=True, embedding_function=embeddings)\n",
        "retriever = db.as_retriever()\n",
        "retriever.search_kwargs['distance_metric'] = 'cos'\n",
        "retriever.search_kwargs['fetch_k'] = 100\n",
        "retriever.search_kwargs['maximal_marginal_relevance'] = True\n",
        "retriever.search_kwargs['k'] = 10\n",
        "\n",
        "def filter(x):\n",
        "    # filter based on source code\n",
        "    if 'com.google' in x['text'].data()['value']:\n",
        "        return False\n",
        "    \n",
        "    # filter based on path e.g. extension\n",
        "    metadata =  x['metadata'].data()['value']\n",
        "    return 'scala' in metadata['source'] or 'py' in metadata['source']\n",
        "\n",
        "### turn on below for custom filtering\n",
        "# retriever.search_kwargs['filter'] = filter\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "model = ChatOpenAI(model_name = 'gpt-3.5-turbo') # switch to 'gpt-4'\n",
        "\n",
        "qa = ConversationalRetrievalChain.from_llm(model,retriever=retriever)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SroFXJ2Wp9QH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56eef7ab-a75d-4d58-f2bd-f49ec595c6c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It is recommended to perform feature scaling (standardization) after splitting the data into training and testing sets. This is to avoid data leakage from the test set into the training set, which can lead to overestimating the performance of the model.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "questions = [\n",
        "    \"do i standardise before spliting the data or after ? \",\n",
        "] \n",
        "chat_history = []\n",
        "result = qa({\"question\": questions[0], \"chat_history\": chat_history})\n",
        "\n",
        "print(result['answer'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "questions = [\n",
        "    \"how do i use the confusion matrix\",\n",
        "] \n",
        "chat_history = []\n",
        "result = qa({\"question\": questions[0], \"chat_history\": chat_history})\n",
        "\n",
        "print(result['answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EPjh60utp3n",
        "outputId": "28080453-c776-4175-d272-08d374be917c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The confusion matrix is a way to evaluate the performance of a classification model. It shows the number of correct and incorrect predictions made by the model compared to the actual outcomes. To use the confusion matrix, you will need to have the predicted values and the true values. You can then use the `table()` function in R to create the matrix. The rows represent the true values, and the columns represent the predicted values. The diagonal represents the number of correct predictions, while the off-diagonal elements represent the incorrect predictions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "   \"do i standardise before spliting the data or after ? \",\n",
        "   \"can you elaborate?\",\n",
        "   \"what function can i use to do this\",\n",
        "   \"can you give me the code that i can apply\"\n",
        "] \n",
        "chat_history = []\n",
        "\n",
        "for question in questions:  \n",
        "    result = qa({\"question\": question, \"chat_history\": chat_history})\n",
        "    chat_history.append((question, result['answer']))\n",
        "    print(f\"-> **Question**: {question} \\n\")\n",
        "    print(f\"**Answer**: {result['answer']} \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79N5VR7yuCYZ",
        "outputId": "c1aa31a9-6c6d-4e2e-927d-a71485c4ca2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-jIsen1ZfuwiI1BuDNa2QybIe on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-> **Question**: do i standardise before spliting the data or after ?  \n",
            "\n",
            "**Answer**: It is generally recommended to standardize the data after splitting into training and test sets. This is because scaling should be applied only on the training data and then the same scaling parameters should be used on the test data. \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-jIsen1ZfuwiI1BuDNa2QybIe on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-jIsen1ZfuwiI1BuDNa2QybIe on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-jIsen1ZfuwiI1BuDNa2QybIe on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-jIsen1ZfuwiI1BuDNa2QybIe on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-jIsen1ZfuwiI1BuDNa2QybIe on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 16.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-jIsen1ZfuwiI1BuDNa2QybIe on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-jIsen1ZfuwiI1BuDNa2QybIe on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-> **Question**: can you elaborate? \n",
            "\n",
            "**Answer**: It is recommended to standardize the data after splitting into training and test sets to avoid information leakage. If the data is standardized before splitting, the mean and variance of the whole dataset is used to transform the test set which is wrong. This may lead to overfitting as the model may perform very well on the training set but poorly on the test set. \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-jIsen1ZfuwiI1BuDNa2QybIe on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-jIsen1ZfuwiI1BuDNa2QybIe on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-jIsen1ZfuwiI1BuDNa2QybIe on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-jIsen1ZfuwiI1BuDNa2QybIe on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-jIsen1ZfuwiI1BuDNa2QybIe on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 16.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-jIsen1ZfuwiI1BuDNa2QybIe on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-jIsen1ZfuwiI1BuDNa2QybIe on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-> **Question**: what function can i use to do this \n",
            "\n",
            "**Answer**: The function that should be used to standardize the data after splitting into training and test sets is \"scale()\". \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-jIsen1ZfuwiI1BuDNa2QybIe on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-jIsen1ZfuwiI1BuDNa2QybIe on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-jIsen1ZfuwiI1BuDNa2QybIe on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-jIsen1ZfuwiI1BuDNa2QybIe on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-jIsen1ZfuwiI1BuDNa2QybIe on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-jIsen1ZfuwiI1BuDNa2QybIe on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 16.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-jIsen1ZfuwiI1BuDNa2QybIe on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-> **Question**: can you give me the code that i can apply \n",
            "\n",
            "**Answer**: To standardize data after splitting into training and test sets, you should use the following code:\n",
            "\n",
            "```\n",
            "training_set[-dependent_variable_column] = scale(training_set[-dependent_variable_column])\n",
            "test_set[-dependent_variable_column] = scale(test_set[-dependent_variable_column])\n",
            "```\n",
            "\n",
            "Replace `dependent_variable_column` with the column index or name of the dependent variable in your dataset. \n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}