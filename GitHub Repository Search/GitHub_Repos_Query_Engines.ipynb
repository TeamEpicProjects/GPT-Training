{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "512d0ad4",
   "metadata": {},
   "source": [
    "## Retreiving information and code from GitHub Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5bddf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd6d210",
   "metadata": {},
   "source": [
    "### OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d77fa1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OPENAI_API_KEY=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\n"
     ]
    }
   ],
   "source": [
    "%env OPENAI_API_KEY=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\n",
    "from llama_index import GPTVectorStoreIndex, GithubRepositoryReader\n",
    "from IPython.display import Markdown, display\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "296361a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GITHUB_TOKEN=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\n"
     ]
    }
   ],
   "source": [
    "%env GITHUB_TOKEN=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\n",
    "github_token = os.environ.get(\"GITHUB_TOKEN\")\n",
    "owner = \"dair-ai\"\n",
    "repo = \"Prompt-Engineering-Guide\"\n",
    "branch = \"main\"\n",
    "\n",
    "documents = GithubRepositoryReader(\n",
    "    github_token=github_token,\n",
    "    owner=owner,\n",
    "    repo=repo,\n",
    "    use_parser=False,\n",
    "    verbose=False,\n",
    ").load_data(branch=branch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8de269",
   "metadata": {},
   "source": [
    "## Quering using default High Level Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff81fe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = GPTVectorStoreIndex.from_documents(documents)\n",
    "\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1435cf05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "Zero-shot prompting is a technique used to enable in-context learning where no examples are provided in the prompt. The model is expected to generate a response based on its understanding of the task. Chain-of-thought (CoT) prompting is a more advanced prompting technique used to address more complex arithmetic, commonsense, and symbolic reasoning tasks. It involves breaking down the problem into steps and providing demonstrations to the model.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(\"What is the difference between Zero shot prompting and Chain of thought prompting?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1b67d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "def set_open_params(\n",
       "    model=\"text-davinci-003\",\n",
       "    temperature=0.7,\n",
       "    max_tokens=256,\n",
       "    top_p=1,\n",
       "    frequency_penalty=0,\n",
       "    presence_penalty=0,\n",
       "):\n",
       "    \"\"\" set openai parameters\"\"\"\n",
       "\n",
       "    openai_params = {}   \n",
       "\n",
       "    openai_params['model'] = model\n",
       "    openai_params['temperature'] = temperature\n",
       "    openai_params['max_tokens'] = max_tokens\n",
       "    openai_params['top_p'] = top_p\n",
       "    openai_params['frequency_penalty'] = frequency_penalty\n",
       "    openai_params['presence_penalty'] = presence_penalty\n",
       "    return openai_params</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(\"Can you give the code for set parameters function?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e2f70e",
   "metadata": {},
   "source": [
    "## Customizing Quering Engine using LLM Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed191483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import LLMPredictor, GPTVectorStoreIndex\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# define LLM\n",
    "llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"))\n",
    "\n",
    "# define prompt helper\n",
    "# set maximum input size\n",
    "max_input_size = 4096\n",
    "# set number of output tokens\n",
    "num_output = 1048\n",
    "# set maximum chunk overlap\n",
    "max_chunk_overlap = 20\n",
    "\n",
    "custom_LLM_index = GPTVectorStoreIndex.from_documents(\n",
    "    documents, llm_predictor=llm_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1d0a281",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_1 = custom_LLM_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c20361f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "def set_open_params(\n",
       "    model=\"text-davinci-003\",\n",
       "    temperature=0.7,\n",
       "    max_tokens=256,\n",
       "    top_p=1,\n",
       "    frequency_penalty=0,\n",
       "    presence_penalty=0,\n",
       "):\n",
       "    \"\"\" set openai parameters\"\"\"\n",
       "\n",
       "    openai_params = {}    \n",
       "\n",
       "    openai_params['model'] = model\n",
       "    openai_params['temperature'] = temperature\n",
       "    openai_params['max_tokens'] = max_tokens\n",
       "    openai_params['top_p'] = top_p\n",
       "    openai_params['frequency_penalty'] = frequency_penalty\n",
       "    openai_params['presence_penalty'] = presence_penalty\n",
       "    return openai_params</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine_1.query(\"Can you give the code for setting parameters function?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "296bd149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "What is the difference between ChatGPT and Text-Davinci-002?\n",
       "\n",
       "Answer: ChatGPT is a more advanced version of the Text-Davinci-002 model. It uses a chat format as input, expecting a series of messages as input and uses those to generate a response. It also supports single-turn tasks similar to what we used with Text-Davinci-002, but it is more efficient and can generate more natural-sounding responses.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine_1.query(\"I'm trying to learn about chatgpt and recently worked with text davinci 002 model.\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aabee8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac87fca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3617156b",
   "metadata": {},
   "source": [
    "## Creating custom Query Engine using Llama index's Retrievers similarity cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e979e4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def set_open_params(\n",
      "    model=\"text-davinci-003\",\n",
      "    temperature=0.7,\n",
      "    max_tokens=256,\n",
      "    top_p=1,\n",
      "    frequency_penalty=0,\n",
      "    presence_penalty=0,\n",
      "):\n",
      "    \"\"\" set openai parameters\"\"\"\n",
      "\n",
      "    openai_params = {}    \n",
      "\n",
      "    openai_params['model'] = model\n",
      "    openai_params['temperature'] = temperature\n",
      "    openai_params['max_tokens'] = max_tokens\n",
      "    openai_params['top_p'] = top_p\n",
      "    openai_params['frequency_penalty'] = frequency_penalty\n",
      "    openai_params['presence_penalty'] = presence_penalty\n",
      "    return openai_params\n"
     ]
    }
   ],
   "source": [
    "from llama_index import (\n",
    "    GPTVectorStoreIndex,\n",
    "    ResponseSynthesizer,\n",
    ")\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.indices.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=custom_LLM_index, \n",
    "    similarity_top_k=2,\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = ResponseSynthesizer.from_args(\n",
    "    node_postprocessors=[\n",
    "        SimilarityPostprocessor(similarity_cutoff=0.7)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "# query\n",
    "response = query_engine.query(\"Can you give the code for setting parameters function?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d32500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d433e11d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
